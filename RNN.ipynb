{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len,names))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "# plt.title('Sequence length distribution')\n",
    "# plt.hist(list(map(len,names)),bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = []\n",
    "for name in names:\n",
    "    for tok in name:\n",
    "        src.append(tok)\n",
    "\n",
    "tokens = set(src)\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens = ',n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {}\n",
    "for tok in tokens:\n",
    "    token_to_id[tok] = tokens.index(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix(names,max_len=None,pad=0,dtype='int32'):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,names))\n",
    "    names_ix = np.zeros([len(names),max_len],dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get,names[i]))\n",
    "        names_ix[i,:len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Concatenate,Dense,Embedding,Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "rnn_num_units = 20\n",
    "embedding_size = 8\n",
    "\n",
    "embed_x = Embedding(n_tokens,embedding_size) # an embedding layer that converts character ids into embeddings\n",
    "\n",
    "#a dense layer that maps input and previous state to new hidden state, [x_t,h_t]->h_t+1\n",
    "random_array = np.random.randn(rnn_num_units + embedding_size, rnn_num_units)\n",
    "weights_0 = tf.Variable(initial_value= np.array(random_array,dtype=np.float32))\n",
    "b_0 = tf.Variable(initial_value=np.zeros((rnn_num_units), dtype='float32'))\n",
    "\n",
    "\n",
    "weights_1 = tf.Variable(initial_value=np.random.randn(rnn_num_units, n_tokens).astype('float32'))\n",
    "b_1 = tf.Variable(initial_value=np.zeros((n_tokens), dtype='float32'))\n",
    "\n",
    "# get_h_next = Dense(rnn_num_units, activation='tanh', kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "\n",
    "#a dense layer that maps current hidden state to probabilities of characters [h_t+1]->P(x_t+1|h_t+1)\n",
    "# get_probas = Dense(n_tokens, activation='softmax', kernel_regularizer=regularizers.l2(0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def rnn_one_step(x_t, h_t):\n",
    "\n",
    "    #convert character id into embedding\n",
    "    x_t_emb = embed_x(tf.reshape(x_t,[-1,1]))[:,0]\n",
    "    \n",
    "    #concatenate x embedding and previous h state\n",
    "    x_and_h = tf.concat([x_t_emb, h_t], axis=-1)\n",
    "    \n",
    "    #compute next state given x_and_h\n",
    "    h_next = tf.nn.tanh(tf.matmul(x_and_h, weights_0) + b_0)\n",
    "    \n",
    "    #get probabilities for language model P(x_next|h_next)\n",
    "    output_probas = tf.nn.softmax(tf.matmul(h_next, weights_1) + b_1)\n",
    "#     output_probas = get_probas(h_next)\n",
    "    \n",
    "    return output_probas,h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder('int32',(MAX_LENGTH, None))\n",
    "batch_size = tf.shape(input_sequence)[1]\n",
    "\n",
    "predicted_probas = []\n",
    "h_prev = tf.zeros([batch_size,rnn_num_units]) #initial hidden state\n",
    "\n",
    "for t in range(MAX_LENGTH):\n",
    "    x_t = input_sequence[t]\n",
    "    probas_next,h_next = rnn_one_step(x_t,h_prev)\n",
    "    \n",
    "    h_prev = h_next\n",
    "    predicted_probas.append(probas_next)\n",
    "    \n",
    "predicted_probas = tf.stack(predicted_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_matrix = tf.reshape(predicted_probas[:-1],[-1,len(tokens)])\n",
    "answers_matrix = tf.one_hot(tf.reshape(input_sequence[1:],[-1]), n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_loss = 0.01*(tf.nn.l2_loss(weights_0) + tf.nn.l2_loss(weights_1))\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=answers_matrix, logits=predictions_matrix)) + reg_loss\n",
    "\n",
    "optimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "s = keras.backend.get_session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    batch = to_matrix(sample(names,64),max_len=MAX_LENGTH)\n",
    "    loss_i,_ = s.run([loss,optimize], {input_sequence:batch})\n",
    "    \n",
    "    \n",
    "    history.append(loss_i)\n",
    "    if (i+1)%100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
